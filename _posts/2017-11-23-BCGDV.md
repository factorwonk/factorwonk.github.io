---
layout: post
title: Clustering and Classification Analysis
---


**Summary**  <br />  <br /> For the unsupervised learning problem I use a K-Means algorithm to distinguish individual clusters of data by maximising the silhouette coefficient. When the dimensions of the original dataset are reduced using PCA, two clusters of datapoints are quite close to one another (similar variances) along the first principal component and are grouped into a single cluster by K-Means. A density based algorithm fares better at identifying three clusters but depending on the parameters certain datapoints are as classed as outliers. The classification task is complicated by an imbalanced class. 70% of the target variable is of Class 1 meaning that a model that classifies every value of the target variable as 1 will have an accuracy of 70%. Since the area under an ROC AUC curve is insensitive to imabalanced classes I use a hypertuned Logistic Classifier to maximize this metric and identify the most important features.

The repository containing code for both sections is [here](https://github.com/factorwonk/bcgdv) and presentation slides are [here]().

## Section 1: Unsupervised Learning (Cluster) Analysis

K-Means is one of the most intuitive and easiest to implement unsupervised learning algorithms to discover subgroups in data. It begins by randomly generating centroids in our dataset, finding the (euclidean) distance from each point to the centroid and recalculating centroid position until stopping criteria are met. The only parameter we need specify is the number of centroids (k). To know whether our clustering is any good, we combine notions of *cohesion* (how far points in a cluster are from their centroid) and *separation* (how far clusters are from each other). Figure 1 shows the silhouette coefficient is maximized when we have two clusters in the provided dataset.

<p align="center">
Figure 1
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvsilcoef.png)

Performing Principal Components Analysis on the original data to reduce it to a two dimensional plane, Figure 2 shows the original dataset on the left and the two clusters identified by K-Means on the right. Here we see one of the shortcomings of K-Means. As the variances of the 'b' and 'c' groups are similar along the first principal component, K-Means identifies it as a single cluster, placing its centroid in between the two groups.

<p align="center">
Figure 2
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvpca2clusters.png)

While it's Silhouette Coefficient is lower, initializing 3 centroids does a marginally better job at identifying three clusters of data. What is apparent here is that 'outlying points' in the 'c' group shift the centroid of that cluster to the top-right of the two dimensional plane.

<p align="center">
Figure 3
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvpca3clusters.png)

While not specifically asked for, out of interest I examine how well a density based clustering algorithm (**DBSCAN**) distinguishes between the three groups. Specifying a minimum neighbor count and a radius, Figure 4 shows this clustering algorithm flag three clusters of data while excluding some of the data points as outliers.

<p align="center">
Figure 4
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvpca3clustersdbscan.png) 

**Advantages and Disadvantages of Clustering Algorithms**

While K-Means is intuitive and less computationally intensive especially if 'k' is kept low, compared to other clusterers, there are drawbacks. The algorithm is scale dependent and not suitable for data with widely varying shapes or densities as it usually relies on the variance of the distribution of the dataset being spherical. Clusters are also assumed to be of similar sizes and we have to specify the number of cluster centers *a priori*.

An alternative is the density-based spatial clustering of applications with noise (DBSCAN) algorithm which groups together closely packed points given a minimum number of neighbors and a threshold radius. Unlike K-Means we do not have to specify the number of clusters a priori, although we do specify a neighbor count and radius. It is computationally more intensive than K-Means making it problematic when scaling up to higher dimensional data. DBSCAN will categorize points as 'noise' if they are located far away from the cluster center.

Finally, top-down hierarchical clustering, begins by dividing the dataset into 2 clusters and repeating this process within each cluster, until all the clusters are too small or similar. Bottom-up hierarchical clustering, starts with each data item being its own cluster, combining two items that are most similar into a larger cluster and repeating this process until all the clusters left are too dissimilar. K-Means clustering works by dividing data into k sets simultaneously. Hierarchical clustering works well with non-spherical data and as the algorithm is deterministic, you end up with the same cluster each time. K-Means on the other hand, begins with a random draw of the centroids and may yield slightly different clustering results on different runs of the algorithm.

## Section 2: Classification Analysis

I decided to use a Logistic Classifier which transforms an underlying linear model using a (sigmoid) function to provide a binary outcome. Identifying features with the highest explanatory power leads to a more intuitive (linear) explanation than using a CART Classifier for example.

**EDA and Preprocessing**

Exploratory Data Analysis reveals 70% of the target variable is Class 1 and 30% is Class 0. This imbalanced class problem means any model that predicts target variable is **all** Class 1 has a 70% accuracy. I use the area under the ROC curve as my performance metric as it remains unaffected by imbalanced classes. 

Figure 5 examines the distribution of all features and target values. Features A21 to A24 are categorical, able to take values A, B or C, and do not appear. I use dummy variables represent these categorical points, dropping A21_A, A22_A, A23_A and A24_A to avoid the *dummy variable trap*. These binarized features are then added back to the quantitative features: A1 - A20. 

<p align="center">
Figure 5
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvclassboxplots.png) 

Note the scales of variables A1-A3 are different to the remainng variables and that A2 in particular has a large number of outliers. To tackle the differing scales as well as the outliers in the distribution, I apply a robust standardizer to scale these features. Finally I note the minimum values of A7 and A8 are 0 whereas the minimum values for all other non-binary features are 1. I assume that zero is **not** a placeholder for NULL values and actually represents the value of zero.

**Logistic Classifier**

As a baseline model, I use a Dummy Classification model which takes the mean of each (standardized) feature as a regressor and tries to classify the target variable into 1s and 0s. The Dummy classifier has an accuracy score of 56.364% and the area under the ROC curve is 0.503%. Note that this underperforms a mode that simply predicts the target variable is 1.

I employ a Logistic Classifier which transforms a linear model of all 24 features using a sigmoid function to predict whether the target variable will be 1 or 0. Running a Stratified 10 fold cross validation the model has an average ROC_AUC of 75.2%. I run a gridsearch optimization to determine the optimal value of the inverse regularization parameter 'C' that maximizes ROC_AUC to 76.1%. The optimal value of 'C' is 0.1 which is small and suggests a strong degree of regularization.

**Logistic Classifier**

<p align="center">
Figure 6
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvclassrocauc.png) 

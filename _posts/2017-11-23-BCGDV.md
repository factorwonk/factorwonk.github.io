---
layout: post
title: Clustering and Classification Analysis
---


**Summary**  <br />  <br /> For the unsupervised learning problem I use a K-Means algorithm to distinguish individual clusters of data by maximising the silhouette coefficient. When the dimensions of the original dataset are reduced using PCA, two clusters of datapoints are quite close to one another (similar variances) along the first principal component and are grouped into a single cluster by K-Means. A density based algorithm fares better at identifying three clusters but depending on the parameters certain datapoints are as classed as outliers. The classification task is complicated by an imbalanced class. 70% of the target variable is of Class 1 meaning that a model that classifies every value of the target variable as 1 will have an accuracy of 70%. Since the area under an ROC AUC curve is insensitive to imabalanced classes I use a hypertuned Logistic Classifier to maximize this metric and identify the most important features.

## Q1. Unsupervised Learning (Clustering) Analysis

K-Means is one of the most intuitive and easiest to implement unsupervised learning algorithms to discover subgroups in data. It begins by randomly generating centroids in our dataset, finding the (euclidean) distance from each point to the centroid and recalculating centroid position until stopping criteria are met. The only parameter we need specify is the number of centroids (k). To know whether our clustering is any good, we combine notions of *cohesion* (how far points in a cluster are from their centroid) and *separation* (how far clusters are from each other). Figure 1 shows the silhouette coefficient is maximized when we have two clusters in the provided dataset.

<p align="center">
Figure 1
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvsilcoef.png)

While K-Means is intuitive and less computationally intensive especially if 'k' is kept low, compared to other clusterers, there are drawbacks. The algorithm is scale dependent and not suitable for data with widely varying shapes or densities as it usually relies on the variance of the distribution of the dataset being spherical. Clusters are also assumed to be of similar sizes and we have to specify the number of cluster centers *a priori*.

Performing Principal Components Analysis on the original data to reduce it to a two dimensional plane, Figure 2 shows the original dataset on the left and the two clusters identified by K-Means on the right. Here we see one of the shortcomings of K-Means. As the variances of the 'b' and 'c' groups are similar along the first principal component, K-Means identifies it as a single cluster, placing its centroid in between the two groups.

<p align="center">
Figure 2
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvpca2clusters.png)

While it's Silhouette Coefficient is lower, initializing 3 centroids does a marginally better job at identifying three clusters of data. What is apparent here is that 'outlying points' in the 'c' group shift the centroid of that cluster to the top-right of the two dimensional plane.

<p align="center">
Figure 3
</p>
![_config.yml]({{ site.baseurl }}/images/bcgdvpca3clusters.png)


The repository containing my analysis is [here](https://github.com/factorwonk/bcgdv) and presentation slides are [here](). 

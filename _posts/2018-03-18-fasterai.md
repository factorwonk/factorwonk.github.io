---
layout: post
title: Fast(er) ai
---

**Jeremy Howard's fast.ai Study Group, 2018** <br /> <br /> In February, I was invited by Sydney Machine Learning's [Paul Conyngham](https://twitter.com/paul_conyngham), to join a Saturday morning study group to tackle the first seven weeks of Jeremy Howard's [fast.ai](http://course.fast.ai/start.html) **Deep Learning for Coders** MOOC, under the kind auspices of [The Domain Group](https://www.domain.com.au/). I was excited to be part of the study group, as the course came highly recommended by my mentor [Greg Baker](https://www.linkedin.com/in/solresol/). 

In this ongoing post I will try to document everything I learned. While we studied the material together, the Jupyter notebooks here represent **my own work** at extending concepts covered in the MOOC with [kaggle](https://www.kaggle.com) sourced datasets and competitions. The course uses the fast.ai library as a wrapper for [PyTorch](http://pytorch.org/) an open source, GPU optimized library for deep learning, developed by [Facebook's AI team](https://www.infoworld.com/article/3159120/artificial-intelligence/facebook-brings-gpu-powered-machine-learning-to-python.html).

**Week 1: Facial Recognition Classifier - Tony Stark or Elon Musk?**<br /> <br />Robert Downey Jr reinvigorated his career with his portrayal of Tony Stark in [Marvel's Iron Man (2008)](http://marvel.com/movies/movie/19/iron_man) movies. A little known fact is that Downey (hereafter: RDJ) based his portrayal of the genious, billionaire, playboy, philanthropist on serial entrepreneur [Elon Musk](https://www.theguardian.com/technology/2018/feb/09/elon-musk-the-real-life-iron-man). I wanted to use a Convolutional Neural Network, trained on [ImageNet data](https://en.wikipedia.org/wiki/ImageNet) to train a single label image classification model to distinguish between photos of RDJ and Elon Musk.

The input for this convolutional neural network will be photos of Elon Musk and Robert Downey Jr. Each photo will be transformed into a **rank 3 tensor** with dimensions *height x width x 3* where 3 represents each of the Red, Blue and Green channels that combine to form a pixel color. Each pixel in an image will have a value from 0 (black) to 255 (white).

Our output activation layer for this Convolutional Neural Network will be a [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) function which transforms the result of the previous layer into a **0** (RDJ) or **1** (Musk) classification, based on a preset threshold. Here we use the threshold 0.5, meaning that any image which is processed by the penultimate layer of our CNN and has an output value greater than or equal to 0.5 will be assigned to Musk otherwise Downey.

*Figure 1: Rank Tensor 3 representation of Elon Musk's photo*

![_config.yml]({{ site.baseurl }}/images/elonmuskimage.png)

I use the [Resnet34](https://arxiv.org/abs/1512.03385) architecture for facial recognition based on the paper by He et al (2015), trained for 10 epochs with a learning rate = 0.01, pre-trained on the ImageNet database. The learning rate is a hyperparameter representing how quickly or how slowly we want to update the weights of our model.

A first pass using this model reveals the images that the architecture is more sure about

![_config.yml]({{ site.baseurl }}/images/elonmuskmoresure.png)

and the images it's not quite so sure about. Note that the architecture seems to be having a tougher time identifying Elon as opposed to Downey. The model has an accuracy of around 70%.

![_config.yml]({{ site.baseurl }}/images/elonmusklesssure.png)

We're going to make a few quick improvements to the Resnet30 model by:

1. Changing the learning rate - this is our key hyperparameter
2. Augmenting the data

In order to determine the ideal learning rate, we use the he technique developed in the 2015 paper [Cyclical Learning Rates for Training Neural Networks](https://arxiv.org/abs/1506.01186), where we keep increasing the learning rate from a very small value, until the loss stops decreasing. We can plot the learning rate across batches to see what this looks like.

*Figure 2: Chart of Learning Rate*

![_config.yml]({{ site.baseurl }}/images/elonmusklr.png)

For our untrained model, we start with a learning rate = 0.001 at which point the loss is clearly improving and not yet at a minimum. Simply changing the learning rate and training for more epochs will result in overfitting.  One way to fix this is to effectively create more data, through data augmentation. This refers to randomly changing the images in ways that shouldn't impact their interpretation, such as horizontal flipping, zooming, and rotating.

*Figure 3: Augmenting the Images*

![_config.yml]({{ site.baseurl }}/images/elonmuskaugmentation.png)

This is similar to bootstrapping where we create new pieces of data using the same distribution of the original data. Now we re-train the model using our new learning rate and by adding our augmented data with the training model. Training over 10 epochs we get to an accuracy of 95%. The confusion matrix below shows that our model is now getting all photos of Robert Downey Jr correct in the test set and is only tripping up on a single photo of Elon Musk.

*Figure 4: Confusion Matrix shows accuracy of Binary Image Classifier*

![_config.yml]({{ site.baseurl }}/images/elonmuskconfusionmatrix.png)

The complete notebook for the Single Label Image Classifier is available [here](https://github.com/factorwonk/fastai/blob/master/adas-lesson1-genius-billionaire-playboy-philanthropist.ipynb)

----
****

**Week 2: Multi-Label Classification - Breaking into the Top 20% of the Kaggle Seedlings Competition**<br /> <br />While the previous Resnet34 model was used for single label classification, an image is either an image of Elon Musk or it isn't, a larger architecture **ResNet50** (also described in the paper by Kaimeng He) is shown to have greater accuracy when used for multi-label classification. Here I entered the Kaggle [plant seedlings competition](https://www.kaggle.com/c/plant-seedlings-classification) to distinguish between twelve different species of plant seedlings based on their images. I trained a ResNet50 classifier on a training set of labeled images and used that classifier to identify plants on unlabelled data. 

The biggest difference between the images in this competition and in the previous example are that they would not have been part of the ImageNet database. Instead of relying on the pre-trained ResNet34 architecture that was optimized for ImageNet like data (include facial recognition) larger architectures such as ResNet50, ResNext50 and ResNet101 may prove more accurate.

![_config.yml]({{ site.baseurl }}/images/plantseedlingsimage.png)

This time our final activation layer is not a sigmoid function as we're trying to distinguish between 12 plant seedling categories. Instead we use a [softmax](https://en.wikipedia.org/wiki/Softmax_function) layer as the final activation function. As a result the final output is as 12x1 vector for each image where the value of the vector is highest for a particular class of a plant seedling.

*Figure 5: Top Down Augmentation of Plant Seedlings *

![_config.yml]({{ site.baseurl }}/images/plantseedlingsaugmentation.png)

Once again we perform augmentations on the phots of the plant seedlings to give our multi-label classifier more data to work with. As our dataset is not a part of the ImageNet dataset, we use a slightly larger architecture and perform top down augmentations on our base dataset. We are able to about 97% classification accuracy and even place in the top 20% of an Kaggle competition.

![_config.yml]({{ site.baseurl }}/images/plantseedlingsrank.png)



The complete notebook for the Multi-Label Image Classifier is available [here](https://github.com/factorwonk/fastai/blob/master/adas-lesson2-seedlings-final.ipynb)

----
****
